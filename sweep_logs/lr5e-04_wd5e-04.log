Loading multiple subjects from: data
Found 27 .csv files

Loading: B0101T.csv
Loading CSV file: data\B0101T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0102T.csv
Loading CSV file: data\B0102T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0103T.csv
Loading CSV file: data\B0103T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0201T.csv
Loading CSV file: data\B0201T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0202T.csv
Loading CSV file: data\B0202T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0203T.csv
Loading CSV file: data\B0203T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0301T.csv
Loading CSV file: data\B0301T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0302T.csv
Loading CSV file: data\B0302T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0303T.csv
Loading CSV file: data\B0303T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0401T.csv
Loading CSV file: data\B0401T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0402T.csv
Loading CSV file: data\B0402T.csv
Created 140 windows with shape (140, 6, 1000)
Label distribution: [70 70]

Loading: B0403T.csv
Loading CSV file: data\B0403T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0501T.csv
Loading CSV file: data\B0501T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0502T.csv
Loading CSV file: data\B0502T.csv
Created 140 windows with shape (140, 6, 1000)
Label distribution: [70 70]

Loading: B0503T.csv
Loading CSV file: data\B0503T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0601T.csv
Loading CSV file: data\B0601T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0602T.csv
Loading CSV file: data\B0602T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0603T.csv
Loading CSV file: data\B0603T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0701T.csv
Loading CSV file: data\B0701T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0702T.csv
Loading CSV file: data\B0702T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0703T.csv
Loading CSV file: data\B0703T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0801T.csv
Loading CSV file: data\B0801T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0802T.csv
Loading CSV file: data\B0802T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0803T.csv
Loading CSV file: data\B0803T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0901T.csv
Loading CSV file: data\B0901T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0902T.csv
Loading CSV file: data\B0902T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0903T.csv
Loading CSV file: data\B0903T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Total data loaded: 3680 epochs
Loaded data: (3680, 6, 1000), labels: (3680,)
Label distribution: [1840 1840]

Using device: cpu
Training samples: 2576
Validation samples: 552
Test samples: 552
Using 6 EEG channels
Batch size: 32, Epochs: 120

============================================================
STARTING TRAINING
============================================================
Training with 2576 samples (balanced sampler), validating with 552 samples
Using device: cpu
Model: SimpleEEGNet1D
Optimizer: AdamW with lr=0.0005, weight_decay=0.0005
------------------------------------------------------------
Class counts: [1288.0, 1288.0] | Class weights: [1.0, 1.0]
Epoch 1/40: train_loss=0.1818, train_acc=0.5050, train_kappa=0.0084, val_acc=0.5072, kappa=0.0145, lr=0.000141
  >>> New best kappa: 0.0145
Epoch 2/40: train_loss=0.1796, train_acc=0.5043, train_kappa=0.0079, val_acc=0.5634, kappa=0.1268, lr=0.000261
  >>> New best kappa: 0.1268
Epoch 3/40: train_loss=0.1752, train_acc=0.5303, train_kappa=0.0604, val_acc=0.5942, kappa=0.1884, lr=0.000452
  >>> New best kappa: 0.1884
Epoch 4/40: train_loss=0.1720, train_acc=0.5536, train_kappa=0.1072, val_acc=0.6322, kappa=0.2645, lr=0.000701
  >>> New best kappa: 0.2645
Epoch 5/40: train_loss=0.1724, train_acc=0.5555, train_kappa=0.1096, val_acc=0.6232, kappa=0.2464, lr=0.000991
Epoch 6/40: train_loss=0.1685, train_acc=0.5765, train_kappa=0.1509, val_acc=0.5978, kappa=0.1957, lr=0.001302
Epoch 7/40: train_loss=0.1663, train_acc=0.5908, train_kappa=0.1787, val_acc=0.6522, kappa=0.3043, lr=0.001613
  >>> New best kappa: 0.3043
Epoch 8/40: train_loss=0.1631, train_acc=0.5994, train_kappa=0.1923, val_acc=0.6105, kappa=0.2210, lr=0.001902
Epoch 9/40: train_loss=0.1622, train_acc=0.5811, train_kappa=0.1623, val_acc=0.6377, kappa=0.2754, lr=0.002151
Epoch 10/40: train_loss=0.1607, train_acc=0.6285, train_kappa=0.2555, val_acc=0.6649, kappa=0.3297, lr=0.002341
  >>> New best kappa: 0.3297
Epoch 11/40: train_loss=0.1573, train_acc=0.6110, train_kappa=0.2221, val_acc=0.6431, kappa=0.2862, lr=0.002460
Epoch 12/40: train_loss=0.1553, train_acc=0.6398, train_kappa=0.2776, val_acc=0.6775, kappa=0.3551, lr=0.002500
  >>> New best kappa: 0.3551
Epoch 13/40: train_loss=0.1526, train_acc=0.6452, train_kappa=0.2904, val_acc=0.6667, kappa=0.3333, lr=0.002492
Epoch 14/40: train_loss=0.1504, train_acc=0.6413, train_kappa=0.2822, val_acc=0.6630, kappa=0.3261, lr=0.002468
Epoch 15/40: train_loss=0.1526, train_acc=0.6592, train_kappa=0.3173, val_acc=0.6884, kappa=0.3768, lr=0.002429
  >>> New best kappa: 0.3768
Epoch 16/40: train_loss=0.1505, train_acc=0.6436, train_kappa=0.2859, val_acc=0.6757, kappa=0.3514, lr=0.002375
Epoch 17/40: train_loss=0.1486, train_acc=0.6370, train_kappa=0.2737, val_acc=0.6993, kappa=0.3986, lr=0.002307
  >>> New best kappa: 0.3986
Epoch 18/40: train_loss=0.1504, train_acc=0.6514, train_kappa=0.3025, val_acc=0.6902, kappa=0.3804, lr=0.002226
Epoch 19/40: train_loss=0.1461, train_acc=0.6568, train_kappa=0.3126, val_acc=0.6975, kappa=0.3949, lr=0.002133
Epoch 20/40: train_loss=0.1461, train_acc=0.6382, train_kappa=0.2760, val_acc=0.7047, kappa=0.4094, lr=0.002028
  >>> New best kappa: 0.4094
Epoch 21/40: train_loss=0.1474, train_acc=0.6452, train_kappa=0.2896, val_acc=0.7047, kappa=0.4094, lr=0.001914
Epoch 22/40: train_loss=0.1454, train_acc=0.6759, train_kappa=0.3514, val_acc=0.6866, kappa=0.3732, lr=0.001791
Epoch 23/40: train_loss=0.1459, train_acc=0.6557, train_kappa=0.3113, val_acc=0.6866, kappa=0.3732, lr=0.001661
Epoch 24/40: train_loss=0.1476, train_acc=0.6584, train_kappa=0.3160, val_acc=0.7156, kappa=0.4312, lr=0.001526
  >>> New best kappa: 0.4312
Epoch 25/40: train_loss=0.1471, train_acc=0.6390, train_kappa=0.2767, val_acc=0.6993, kappa=0.3986, lr=0.001388
Epoch 26/40: train_loss=0.1483, train_acc=0.6572, train_kappa=0.3138, val_acc=0.7192, kappa=0.4384, lr=0.001248
  >>> New best kappa: 0.4384
Epoch 27/40: train_loss=0.1437, train_acc=0.6774, train_kappa=0.3491, val_acc=0.7029, kappa=0.4058, lr=0.001108
Epoch 28/40: train_loss=0.1388, train_acc=0.6786, train_kappa=0.3564, val_acc=0.7083, kappa=0.4167, lr=0.000970
Epoch 29/40: train_loss=0.1467, train_acc=0.6432, train_kappa=0.2865, val_acc=0.7120, kappa=0.4239, lr=0.000836
Epoch 30/40: train_loss=0.1426, train_acc=0.6716, train_kappa=0.3428, val_acc=0.6993, kappa=0.3986, lr=0.000706
Epoch 31/40: train_loss=0.1420, train_acc=0.6724, train_kappa=0.3443, val_acc=0.7083, kappa=0.4167, lr=0.000584
Epoch 32/40: train_loss=0.1378, train_acc=0.6790, train_kappa=0.3579, val_acc=0.7011, kappa=0.4022, lr=0.000469
Epoch 33/40: train_loss=0.1393, train_acc=0.6797, train_kappa=0.3589, val_acc=0.7047, kappa=0.4094, lr=0.000365
Epoch 34/40: train_loss=0.1416, train_acc=0.6817, train_kappa=0.3633, val_acc=0.7029, kappa=0.4058, lr=0.000272
Epoch 35/40: train_loss=0.1359, train_acc=0.6797, train_kappa=0.3586, val_acc=0.7083, kappa=0.4167, lr=0.000191
Epoch 36/40: train_loss=0.1388, train_acc=0.6712, train_kappa=0.3424, val_acc=0.7138, kappa=0.4275, lr=0.000123
Epoch 37/40: train_loss=0.1370, train_acc=0.6607, train_kappa=0.3207, val_acc=0.7047, kappa=0.4094, lr=0.000070
Epoch 38/40: train_loss=0.1370, train_acc=0.6902, train_kappa=0.3804, val_acc=0.7083, kappa=0.4167, lr=0.000031

Early stopping at epoch 38 (no improvement for 12 epochs)

Training completed. Best kappa: 0.4384

============================================================
EVALUATING ON TEST SET
============================================================

============================================================
FINAL TEST RESULTS:
============================================================
  Accuracy: 0.7500 (75.00%)
  Cohen's Kappa: 0.5000

Confusion Matrix:
[[204  72]
 [ 66 210]]
============================================================

