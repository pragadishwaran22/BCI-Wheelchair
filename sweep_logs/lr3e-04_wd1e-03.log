Loading multiple subjects from: data
Found 27 .csv files

Loading: B0101T.csv
Loading CSV file: data\B0101T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0102T.csv
Loading CSV file: data\B0102T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0103T.csv
Loading CSV file: data\B0103T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0201T.csv
Loading CSV file: data\B0201T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0202T.csv
Loading CSV file: data\B0202T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0203T.csv
Loading CSV file: data\B0203T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0301T.csv
Loading CSV file: data\B0301T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0302T.csv
Loading CSV file: data\B0302T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0303T.csv
Loading CSV file: data\B0303T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0401T.csv
Loading CSV file: data\B0401T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0402T.csv
Loading CSV file: data\B0402T.csv
Created 140 windows with shape (140, 6, 1000)
Label distribution: [70 70]

Loading: B0403T.csv
Loading CSV file: data\B0403T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0501T.csv
Loading CSV file: data\B0501T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0502T.csv
Loading CSV file: data\B0502T.csv
Created 140 windows with shape (140, 6, 1000)
Label distribution: [70 70]

Loading: B0503T.csv
Loading CSV file: data\B0503T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0601T.csv
Loading CSV file: data\B0601T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0602T.csv
Loading CSV file: data\B0602T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0603T.csv
Loading CSV file: data\B0603T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0701T.csv
Loading CSV file: data\B0701T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0702T.csv
Loading CSV file: data\B0702T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0703T.csv
Loading CSV file: data\B0703T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0801T.csv
Loading CSV file: data\B0801T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0802T.csv
Loading CSV file: data\B0802T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0803T.csv
Loading CSV file: data\B0803T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0901T.csv
Loading CSV file: data\B0901T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0902T.csv
Loading CSV file: data\B0902T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0903T.csv
Loading CSV file: data\B0903T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Total data loaded: 3680 epochs
Loaded data: (3680, 6, 1000), labels: (3680,)
Label distribution: [1840 1840]

Using device: cpu
Training samples: 2576
Validation samples: 552
Test samples: 552
Using 6 EEG channels
Batch size: 32, Epochs: 120

============================================================
STARTING TRAINING
============================================================
Training with 2576 samples (balanced sampler), validating with 552 samples
Using device: cpu
Model: SimpleEEGNet1D
Optimizer: AdamW with lr=0.0003, weight_decay=0.001
------------------------------------------------------------
Class counts: [1288.0, 1288.0] | Class weights: [1.0, 1.0]
Epoch 1/40: train_loss=0.1786, train_acc=0.4965, train_kappa=-0.0104, val_acc=0.5036, kappa=0.0072, lr=0.000085
  >>> New best kappa: 0.0072
Epoch 2/40: train_loss=0.1768, train_acc=0.4953, train_kappa=-0.0093, val_acc=0.5290, kappa=0.0580, lr=0.000157
  >>> New best kappa: 0.0580
Epoch 3/40: train_loss=0.1771, train_acc=0.5140, train_kappa=0.0255, val_acc=0.5833, kappa=0.1667, lr=0.000271
  >>> New best kappa: 0.1667
Epoch 4/40: train_loss=0.1749, train_acc=0.5217, train_kappa=0.0434, val_acc=0.6268, kappa=0.2536, lr=0.000421
  >>> New best kappa: 0.2536
Epoch 5/40: train_loss=0.1710, train_acc=0.5648, train_kappa=0.1296, val_acc=0.6087, kappa=0.2174, lr=0.000595
Epoch 6/40: train_loss=0.1700, train_acc=0.5528, train_kappa=0.1012, val_acc=0.6178, kappa=0.2355, lr=0.000781
Epoch 7/40: train_loss=0.1650, train_acc=0.5811, train_kappa=0.1623, val_acc=0.6250, kappa=0.2500, lr=0.000968
Epoch 8/40: train_loss=0.1645, train_acc=0.5761, train_kappa=0.1521, val_acc=0.6232, kappa=0.2464, lr=0.001141
Epoch 9/40: train_loss=0.1639, train_acc=0.5951, train_kappa=0.1904, val_acc=0.5688, kappa=0.1377, lr=0.001290
Epoch 10/40: train_loss=0.1608, train_acc=0.6048, train_kappa=0.2080, val_acc=0.6341, kappa=0.2681, lr=0.001405
  >>> New best kappa: 0.2681
Epoch 11/40: train_loss=0.1600, train_acc=0.6118, train_kappa=0.2236, val_acc=0.6014, kappa=0.2029, lr=0.001476
Epoch 12/40: train_loss=0.1565, train_acc=0.6083, train_kappa=0.2164, val_acc=0.6377, kappa=0.2754, lr=0.001500
  >>> New best kappa: 0.2754
Epoch 13/40: train_loss=0.1562, train_acc=0.6289, train_kappa=0.2574, val_acc=0.7083, kappa=0.4167, lr=0.001495
  >>> New best kappa: 0.4167
Epoch 14/40: train_loss=0.1549, train_acc=0.6386, train_kappa=0.2768, val_acc=0.7065, kappa=0.4130, lr=0.001481
Epoch 15/40: train_loss=0.1538, train_acc=0.6277, train_kappa=0.2554, val_acc=0.6920, kappa=0.3841, lr=0.001458
Epoch 16/40: train_loss=0.1544, train_acc=0.6304, train_kappa=0.2599, val_acc=0.7101, kappa=0.4203, lr=0.001425
  >>> New best kappa: 0.4203
Epoch 17/40: train_loss=0.1520, train_acc=0.6401, train_kappa=0.2799, val_acc=0.7101, kappa=0.4203, lr=0.001384
Epoch 18/40: train_loss=0.1498, train_acc=0.6339, train_kappa=0.2678, val_acc=0.7065, kappa=0.4130, lr=0.001336
Epoch 19/40: train_loss=0.1419, train_acc=0.6467, train_kappa=0.2930, val_acc=0.7101, kappa=0.4203, lr=0.001280
Epoch 20/40: train_loss=0.1480, train_acc=0.6460, train_kappa=0.2908, val_acc=0.7065, kappa=0.4130, lr=0.001217
Epoch 21/40: train_loss=0.1471, train_acc=0.6487, train_kappa=0.2964, val_acc=0.6993, kappa=0.3986, lr=0.001148
Epoch 22/40: train_loss=0.1491, train_acc=0.6661, train_kappa=0.3314, val_acc=0.7174, kappa=0.4348, lr=0.001074
  >>> New best kappa: 0.4348
Epoch 23/40: train_loss=0.1415, train_acc=0.6755, train_kappa=0.3507, val_acc=0.7355, kappa=0.4710, lr=0.000997
  >>> New best kappa: 0.4710
Epoch 24/40: train_loss=0.1445, train_acc=0.6456, train_kappa=0.2911, val_acc=0.7373, kappa=0.4746, lr=0.000916
  >>> New best kappa: 0.4746
Epoch 25/40: train_loss=0.1407, train_acc=0.6972, train_kappa=0.3942, val_acc=0.7065, kappa=0.4130, lr=0.000833
Epoch 26/40: train_loss=0.1416, train_acc=0.6805, train_kappa=0.3610, val_acc=0.7083, kappa=0.4167, lr=0.000749
Epoch 27/40: train_loss=0.1409, train_acc=0.6394, train_kappa=0.2784, val_acc=0.7101, kappa=0.4203, lr=0.000665
Epoch 28/40: train_loss=0.1428, train_acc=0.6557, train_kappa=0.3094, val_acc=0.7120, kappa=0.4239, lr=0.000582
Epoch 29/40: train_loss=0.1395, train_acc=0.6821, train_kappa=0.3637, val_acc=0.7047, kappa=0.4094, lr=0.000501
Epoch 30/40: train_loss=0.1444, train_acc=0.6739, train_kappa=0.3478, val_acc=0.7246, kappa=0.4493, lr=0.000424
Epoch 31/40: train_loss=0.1446, train_acc=0.6638, train_kappa=0.3272, val_acc=0.7083, kappa=0.4167, lr=0.000350
Epoch 32/40: train_loss=0.1402, train_acc=0.6821, train_kappa=0.3621, val_acc=0.7283, kappa=0.4565, lr=0.000282
Epoch 33/40: train_loss=0.1435, train_acc=0.6460, train_kappa=0.2921, val_acc=0.7210, kappa=0.4420, lr=0.000219
Epoch 34/40: train_loss=0.1367, train_acc=0.6902, train_kappa=0.3804, val_acc=0.7228, kappa=0.4457, lr=0.000163
Epoch 35/40: train_loss=0.1372, train_acc=0.6743, train_kappa=0.3482, val_acc=0.7120, kappa=0.4239, lr=0.000114
Epoch 36/40: train_loss=0.1395, train_acc=0.6828, train_kappa=0.3659, val_acc=0.7301, kappa=0.4601, lr=0.000074

Early stopping at epoch 36 (no improvement for 12 epochs)

Training completed. Best kappa: 0.4746

============================================================
EVALUATING ON TEST SET
============================================================

============================================================
FINAL TEST RESULTS:
============================================================
  Accuracy: 0.7482 (74.82%)
  Cohen's Kappa: 0.4964

Confusion Matrix:
[[181  95]
 [ 44 232]]
============================================================

