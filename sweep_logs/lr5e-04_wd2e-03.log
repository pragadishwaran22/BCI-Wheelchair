Loading multiple subjects from: data
Found 27 .csv files

Loading: B0101T.csv
Loading CSV file: data\B0101T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0102T.csv
Loading CSV file: data\B0102T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0103T.csv
Loading CSV file: data\B0103T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0201T.csv
Loading CSV file: data\B0201T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0202T.csv
Loading CSV file: data\B0202T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0203T.csv
Loading CSV file: data\B0203T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0301T.csv
Loading CSV file: data\B0301T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0302T.csv
Loading CSV file: data\B0302T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0303T.csv
Loading CSV file: data\B0303T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0401T.csv
Loading CSV file: data\B0401T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0402T.csv
Loading CSV file: data\B0402T.csv
Created 140 windows with shape (140, 6, 1000)
Label distribution: [70 70]

Loading: B0403T.csv
Loading CSV file: data\B0403T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0501T.csv
Loading CSV file: data\B0501T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0502T.csv
Loading CSV file: data\B0502T.csv
Created 140 windows with shape (140, 6, 1000)
Label distribution: [70 70]

Loading: B0503T.csv
Loading CSV file: data\B0503T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0601T.csv
Loading CSV file: data\B0601T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0602T.csv
Loading CSV file: data\B0602T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0603T.csv
Loading CSV file: data\B0603T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0701T.csv
Loading CSV file: data\B0701T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0702T.csv
Loading CSV file: data\B0702T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0703T.csv
Loading CSV file: data\B0703T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0801T.csv
Loading CSV file: data\B0801T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0802T.csv
Loading CSV file: data\B0802T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0803T.csv
Loading CSV file: data\B0803T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Loading: B0901T.csv
Loading CSV file: data\B0901T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0902T.csv
Loading CSV file: data\B0902T.csv
Created 120 windows with shape (120, 6, 1000)
Label distribution: [60 60]

Loading: B0903T.csv
Loading CSV file: data\B0903T.csv
Created 160 windows with shape (160, 6, 1000)
Label distribution: [80 80]

Total data loaded: 3680 epochs
Loaded data: (3680, 6, 1000), labels: (3680,)
Label distribution: [1840 1840]

Using device: cpu
Training samples: 2576
Validation samples: 552
Test samples: 552
Using 6 EEG channels
Batch size: 32, Epochs: 120

============================================================
STARTING TRAINING
============================================================
Training with 2576 samples (balanced sampler), validating with 552 samples
Using device: cpu
Model: SimpleEEGNet1D
Optimizer: AdamW with lr=0.0005, weight_decay=0.002
------------------------------------------------------------
Class counts: [1288.0, 1288.0] | Class weights: [1.0, 1.0]
Epoch 1/40: train_loss=0.1849, train_acc=0.5167, train_kappa=0.0325, val_acc=0.5399, kappa=0.0797, lr=0.000141
  >>> New best kappa: 0.0797
Epoch 2/40: train_loss=0.1786, train_acc=0.5116, train_kappa=0.0222, val_acc=0.5453, kappa=0.0906, lr=0.000261
  >>> New best kappa: 0.0906
Epoch 3/40: train_loss=0.1759, train_acc=0.5155, train_kappa=0.0311, val_acc=0.5779, kappa=0.1558, lr=0.000452
  >>> New best kappa: 0.1558
Epoch 4/40: train_loss=0.1727, train_acc=0.5505, train_kappa=0.1010, val_acc=0.5670, kappa=0.1341, lr=0.000701
Epoch 5/40: train_loss=0.1700, train_acc=0.5617, train_kappa=0.1229, val_acc=0.5580, kappa=0.1159, lr=0.000991
Epoch 6/40: train_loss=0.1705, train_acc=0.5644, train_kappa=0.1261, val_acc=0.6431, kappa=0.2862, lr=0.001302
  >>> New best kappa: 0.2862
Epoch 7/40: train_loss=0.1670, train_acc=0.5831, train_kappa=0.1661, val_acc=0.6341, kappa=0.2681, lr=0.001613
Epoch 8/40: train_loss=0.1629, train_acc=0.5881, train_kappa=0.1761, val_acc=0.6141, kappa=0.2283, lr=0.001902
Epoch 9/40: train_loss=0.1590, train_acc=0.6219, train_kappa=0.2412, val_acc=0.6775, kappa=0.3551, lr=0.002151
  >>> New best kappa: 0.3551
Epoch 10/40: train_loss=0.1603, train_acc=0.5924, train_kappa=0.1838, val_acc=0.6938, kappa=0.3877, lr=0.002341
  >>> New best kappa: 0.3877
Epoch 11/40: train_loss=0.1595, train_acc=0.6258, train_kappa=0.2494, val_acc=0.6449, kappa=0.2899, lr=0.002460
Epoch 12/40: train_loss=0.1575, train_acc=0.6254, train_kappa=0.2508, val_acc=0.7101, kappa=0.4203, lr=0.002500
  >>> New best kappa: 0.4203
Epoch 13/40: train_loss=0.1550, train_acc=0.6211, train_kappa=0.2414, val_acc=0.6902, kappa=0.3804, lr=0.002492
Epoch 14/40: train_loss=0.1530, train_acc=0.6277, train_kappa=0.2550, val_acc=0.6830, kappa=0.3659, lr=0.002468
Epoch 15/40: train_loss=0.1498, train_acc=0.6576, train_kappa=0.3152, val_acc=0.7065, kappa=0.4130, lr=0.002429
Epoch 16/40: train_loss=0.1505, train_acc=0.6390, train_kappa=0.2765, val_acc=0.6848, kappa=0.3696, lr=0.002375
Epoch 17/40: train_loss=0.1513, train_acc=0.6464, train_kappa=0.2922, val_acc=0.7011, kappa=0.4022, lr=0.002307
Epoch 18/40: train_loss=0.1445, train_acc=0.6518, train_kappa=0.3033, val_acc=0.6884, kappa=0.3768, lr=0.002226
Epoch 19/40: train_loss=0.1477, train_acc=0.6537, train_kappa=0.3074, val_acc=0.7120, kappa=0.4239, lr=0.002133
  >>> New best kappa: 0.4239
Epoch 20/40: train_loss=0.1466, train_acc=0.6413, train_kappa=0.2825, val_acc=0.7210, kappa=0.4420, lr=0.002028
  >>> New best kappa: 0.4420
Epoch 21/40: train_loss=0.1466, train_acc=0.6689, train_kappa=0.3377, val_acc=0.7083, kappa=0.4167, lr=0.001914
Epoch 22/40: train_loss=0.1419, train_acc=0.6712, train_kappa=0.3424, val_acc=0.7138, kappa=0.4275, lr=0.001791
Epoch 23/40: train_loss=0.1449, train_acc=0.6743, train_kappa=0.3486, val_acc=0.7047, kappa=0.4094, lr=0.001661
Epoch 24/40: train_loss=0.1410, train_acc=0.6848, train_kappa=0.3694, val_acc=0.7011, kappa=0.4022, lr=0.001526
Epoch 25/40: train_loss=0.1450, train_acc=0.6572, train_kappa=0.3145, val_acc=0.6884, kappa=0.3768, lr=0.001388
Epoch 26/40: train_loss=0.1410, train_acc=0.6595, train_kappa=0.3191, val_acc=0.7138, kappa=0.4275, lr=0.001248
Epoch 27/40: train_loss=0.1445, train_acc=0.6537, train_kappa=0.3074, val_acc=0.7337, kappa=0.4674, lr=0.001108
  >>> New best kappa: 0.4674
Epoch 28/40: train_loss=0.1411, train_acc=0.6650, train_kappa=0.3299, val_acc=0.7101, kappa=0.4203, lr=0.000970
Epoch 29/40: train_loss=0.1413, train_acc=0.6580, train_kappa=0.3158, val_acc=0.7192, kappa=0.4384, lr=0.000836
Epoch 30/40: train_loss=0.1390, train_acc=0.6813, train_kappa=0.3623, val_acc=0.7011, kappa=0.4022, lr=0.000706
Epoch 31/40: train_loss=0.1452, train_acc=0.6553, train_kappa=0.3106, val_acc=0.7210, kappa=0.4420, lr=0.000584
Epoch 32/40: train_loss=0.1390, train_acc=0.6739, train_kappa=0.3478, val_acc=0.7391, kappa=0.4783, lr=0.000469
  >>> New best kappa: 0.4783
Epoch 33/40: train_loss=0.1399, train_acc=0.6696, train_kappa=0.3393, val_acc=0.7156, kappa=0.4312, lr=0.000365
Epoch 34/40: train_loss=0.1369, train_acc=0.6832, train_kappa=0.3665, val_acc=0.7246, kappa=0.4493, lr=0.000272
Epoch 35/40: train_loss=0.1375, train_acc=0.6673, train_kappa=0.3341, val_acc=0.7264, kappa=0.4529, lr=0.000191
Epoch 36/40: train_loss=0.1375, train_acc=0.6906, train_kappa=0.3813, val_acc=0.7138, kappa=0.4275, lr=0.000123
Epoch 37/40: train_loss=0.1379, train_acc=0.6700, train_kappa=0.3383, val_acc=0.7246, kappa=0.4493, lr=0.000070
Epoch 38/40: train_loss=0.1383, train_acc=0.6972, train_kappa=0.3943, val_acc=0.7246, kappa=0.4493, lr=0.000031
Epoch 39/40: train_loss=0.1377, train_acc=0.6778, train_kappa=0.3555, val_acc=0.7120, kappa=0.4239, lr=0.000008
Epoch 40/40: train_loss=0.1361, train_acc=0.7069, train_kappa=0.4139, val_acc=0.7228, kappa=0.4457, lr=0.000000

Training completed. Best kappa: 0.4783

============================================================
EVALUATING ON TEST SET
============================================================

============================================================
FINAL TEST RESULTS:
============================================================
  Accuracy: 0.7174 (71.74%)
  Cohen's Kappa: 0.4348

Confusion Matrix:
[[225  51]
 [105 171]]
============================================================

